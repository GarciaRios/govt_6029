---
title: "Lab 3: Linear Regression, Matrices, and predicted Values"
author: "Sergio Garcia-Rios/ Julie George"
output: 
  html_document: 
    highlight: pygments
    theme: simplex
    toc: yes
editor_options: 
  chunk_output_type: console
---
```{r echo = FALSE, results = 'hide'}
knitr::opts_chunk$set(warning = FALSE, message = TRUE, eval = F)
```



## Initial set up, reading in data and initial data exploration

### Load libraries

We will use the following libraries so let's make sure you install them

```{r, message = FALSE}
library(tidyverse)
library(texreg)
library(stargazer)
```


### Data


This dataset is cross sectional data on industrial democracies. Containing:

--------- ----------------------------------------------------------------
`povred`  Percent of citizens lifted out of poverty by taxes and transfers
`enp`     Natural log of effective number of parties
`lnenp`   Natural log of effective number of parties
`maj`     Majoritarian election system dummy
`pr`      Proportional representation dummy
`unam`    Unanimity government dummy (Switzerland)
--------- ----------------------------------------------------------------

Source of data and model Torben Iversen and David Soskice, 2002, ``Why do some democracies redistribute more than others?'' Harvard University.

You can download it and load it (again, most of the time you will not be able to load it directly from the web)

```{r}
iver <- read_csv("https://raw.githubusercontent.com/GarciaRios/govt_6029/master/data/iver.csv")
```

ok, let's take a quick look

```{r}
head(iver) 
```

Now a quick summary

```{r}
summary(iver)
```



## Regression!

The basic command for linear regression in R is `lm()`. A call to this function takes the generic form illustrated below:
```{r, eval=FALSE}
res <- lm(y ~ x1 + x2 + x3, data = your.dataframe)
```


Let's run a regression using the Iverson and Soskice data.  We save the regression as an object and then print the summary of the results:

**A simple bivariate model:**
Here let's use `povred` as the outcoem variable and `lnenp` as our only covariate

```{r}
lm_bivar <- lm(povred ~ lnenp, data = iver)

summary(lm_bivar)
```

*Challenge:*

* How do we interpret this output?


**A multivariate model:**

```{r}
head(iver)
```


```{r}
lm_multi <- lm(povred ~ lnenp + maj + pr, data = iver)
summary(lm_multi)
```



### Aside on the formula

Note: the first argument to the function is an R formula.  Formulas appear throughout many R functions, and have some special features of their syntax, some of which are illustrated below.

In `lm`, the formula is used to generate the exact $X$ matrix that will be used to estimate the model.  To see the matrix being generated internally by `lm`, add the argument `x = TRUE` to the `lm()` call:
```{r}
lm_cat <- lm(povred ~ lnenp + elec_sys, data = iver, x = TRUE)
lm_cat$x
```



Take a look at the help page for `formula`
```{r eval=FALSE}
?formula
```



To better understand what the formula is doing, let's look at the model matrix one of the more complex formulas above generates, note the inteeraction term and the base terms of this interaction:
```{r}

lm_multi_interact <- lm(povred ~ lnenp * elec_sys, data = iver, x = TRUE)


lm_multi_interact$x
```





### Another aside, now on matrices

Matrices are collections of equal-length vectors in row and column arrangement.  Matrices store information in a rectangular format, so look like data frames, but are less flexible as all data must be the same type (you can't mix character and numeric data, for example).  At first when you start working in R, matrices will be in use behind the scenes more than something you work with much.

As an example, see what happens when we convert our `iver` data frame into a matrix:


```{r}
iver_matrix <- as.matrix(iver)

iver_matrix
```


Why is everything in quotation marks now?

You can index a matrix using square brackets, indicating first the row you want, then the column. This is the same way that you could directly extract specific values from a data frame

```{r}
iver_matrix[2:5, ]
```
A blank before or after the comma indicates all rows or columns, respectively
```{r}
iver_matrix[4, 3]
```

If we leave out the character vectors and convert the data frame to a matrix, then the
matrix will be numeric,
```{r}
as.matrix(select(iver, - cty, - elec_sys))
```



*Challenge:*

1. Use `names()` and `glimpse` (or  `str()` it will do the same as `glimpse` in tbis case) to explore the contents of one of the lm objects you've created. (Look at the help file for `lm` for further details)
2. Extract and save as separate objects:
    a. The coefficients
    b. The residuals (what are the residuals?)
    c. The fitted values (what are the fitted values?)

To extract the coefficients,
```{r, echo=}
coefficients_multi <- lm_multi$coefficients
coefficients_multi

# or

coef(lm_multi)
```
To extract the residuals,
```{r}
residuals_multi <- lm_multi$residuals
residuals_multi

# or
resid(lm_multi)
```


Now fitted values


```{r}
fitted_multi <- lm_multi$fitted.values
fitted_multi
#or
fitted(lm_multi)
```


1. What important information is missing from the `lm()` list?

To extract standard errors of the estimates:
```{r}
se_multi <- lm_multi %>% vcov() %>% diag %>% sqrt
se_multi
```
This calculates the standard errors by calculating the square root of the diagonal of the variance-covariance matrix of the parameters of the model object.  `vcov()` is an example of a function that has a specific "method" for different types of objects: it knows we have an  `lm` object and acts accordingly.

## Fitted values and predictions

hypothetical data
Another way to get the fitted values is with `predict()`:
```{r}
predict(lm_multi)
```

The nice thing about predict is that it will actually let us calculate the expected values from our model for any set of real or hypothetical data with the same X variables:

Here's the general form of a call to predict, giving 95% confidence intervals:
```{r eval=FALSE}
predict(object, #lm object
        newdata, # a data frame with same x vars as data, but new or specific values
        interval = "confidence",
        level = 0.95 #the default
)
```

Let's try this with our model.

*Challenge:*

1. What would we expect the level of poverty reduction to be for a majoritarian country with 2 parties?
2. What would we expect the level of poverty reduction to be for a PR country as it goes from 1 to 5 parties?

*hint (refer to data frame info above for how to create a new dataframe for newdata argument)*
```{r}
predict(lm_cat, newdata = data_frame(lnenp = 2, 
                                     elec_sys = "maj"),
        interval = "confidence")
```

Of course, we can use more than one value of interest
```{r}
predict(lm_cat, newdata = data_frame(lnenp = seq(1:5), elec_sys = "maj"),
        interval = "confidence")
```

Same thing as just reating the new hypothetical data and then just insert it into the predict function
```{r}
hypx <-  data_frame(lnenp = seq(1:5), elec_sys = "maj")
hypx
```

```{r}
predict(lm_cat, newdata = hypx, interval="confidence")
```


## Plotting regression results

Plotting regression results can be even more informative. Information dense, and more intuitive than regression tables!

To plot a regression line (not just using the `lm` smoother in **ggplot2**), you can either fit a line to the observed values of X and the fitted values and CIs from `predict`, or fit a line to hypothetical data to illustrate the estimated relationship (the latter can help you to have smoother confidence intervals where you have fewer observations).

We willuse the 



Let's now calculate expected values of `povred` for each observed level of `lnenp`, setting the covariates to an fixed level, illustrating the effect of a change in `lnenp`, all else equal (for a "typical" respondent, use the mean of the covariates you are keeping fixed).  Remember to keep variable names identical to those in the model!



```{r}
xhyp <- data.frame(lnenp = iver$lnenp, maj = mean(iver$maj),
                   pr = mean(iver$pr)) 
xhyp
```


```{r}
preds <- predict(lm_multi, newdata = xhyp , interval = "confidence")

preds
```

Great! let's now merge (bind) our hypothetical data and these prediciotns it will amke tings easier when we graph

```{r}
pred_plot <- cbind(preds, xhyp)
pred_plot
```

Now we can graph it


```{r}
ggplot(pred_plot, aes(x = lnenp, y = fit,
                      ymin = lwr, ymax =  upr)) +
  geom_line() +
  geom_ribbon(alpha = .3)

```


## Formating regression tables.


There are mutiple package that could help with the prese ntaiton for your regression tables. 


```{r results='asis'}
cat(texreg::htmlreg(list(lm_cat),
                html.tag = FALSE,
                head.tag = FALSE,
                body.tag = FALSE,
                doctype = FALSE))
```

```{r results='asis'}
stargazer::stargazer(lm_cat, type = "html")
```


***

I  use stargazer most of the time but there many of these. Use whateve works for you.

***